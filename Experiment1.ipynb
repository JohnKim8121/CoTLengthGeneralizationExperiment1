{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSkXSVbOb82VXhxtuW/K5p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JohnKim8121/CoTLengthGeneralizationExperiment1/blob/main/Experiment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "P7rJVosdyk-T"
      },
      "outputs": [],
      "source": [
        "# Define the vocabulary and encoding/decoding\n",
        "import string\n",
        "import torch # Import torch here for tensor conversion\n",
        "\n",
        "# Base alphabet tokens\n",
        "alphabet = list(string.ascii_uppercase)  # ['A','B',...,'Z']\n",
        "# Special tokens\n",
        "T1_TOKEN = \"[F1]\"   # denotes ROT13\n",
        "T2_TOKEN = \"[F2]\"   # denotes POS1\n",
        "THINK_TOKEN = \"<think>\"\n",
        "ANSWER_TOKEN = \"<answer>\"\n",
        "endoftext_token = \"<|endoftext|>\"\n",
        "\n",
        "\n",
        "# Construct vocabulary list\n",
        "vocab = alphabet + [T1_TOKEN, T2_TOKEN, THINK_TOKEN, ANSWER_TOKEN,endoftext_token]\n",
        "vocab_size = len(vocab)\n",
        "token_to_id = {tok: i for i, tok in enumerate(vocab)}\n",
        "id_to_token = {i: tok for tok, i in token_to_id.items()}\n",
        "\n",
        "def encode_sequence(seq_tokens):\n",
        "    \"\"\"Encode a sequence of token strings (letters or special markers) to list of ids.\"\"\"\n",
        "    return [token_to_id[token] for token in seq_tokens]\n",
        "\n",
        "def decode_sequence(id_list):\n",
        "    \"\"\"Decode a list of token ids back to token strings.\"\"\"\n",
        "    # Convert tensor elements to integers before using as dictionary keys\n",
        "    return [id_to_token[int(i)] for i in id_list]\n",
        "\n",
        "# Transformation functions\n",
        "def apply_rot(sequence, n=13):\n",
        "    \"\"\"Apply ROT-n to a sequence of letters (list of chars).\"\"\"\n",
        "    result = []\n",
        "    for ch in sequence:\n",
        "        if ch not in token_to_id or ch not in alphabet:\n",
        "            raise ValueError(f\"Unexpected token in sequence: {ch}\")\n",
        "        # shift letter by n\n",
        "        new_idx = (ord(ch) - ord('A') + n) % 26\n",
        "        result.append(chr(ord('A') + new_idx))\n",
        "    return result\n",
        "\n",
        "def apply_pos(sequence, n=1):\n",
        "    \"\"\"Apply cyclic position shift (left rotate by n) to a sequence of letters.\"\"\"\n",
        "    l = len(sequence)\n",
        "    # left rotation by n: element at index i moves to index i-n (mod l) in the result\n",
        "    return [sequence[(i + n) % l] for i in range(l)]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment 1 is training len 3,4,5 to test len 1 and 6."
      ],
      "metadata": {
        "id": "qlKmtse9ZMla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Function to generate a random sequence of given length\n",
        "def random_sequence(length):\n",
        "    return [random.choice(alphabet) for _ in range(length)]\n",
        "\n",
        "# Generate training examples\n",
        "train_examples3 = []\n",
        "train_examples4 = []\n",
        "train_examples5 = []\n",
        "train_lengths = [3, 4, 5]  # in-distribution sequence lengths\n",
        "for length in train_lengths:\n",
        "    # For each length, generate a number of examples (you can adjust count for real training)\n",
        "    for _ in range(10000):  # e.g., 1000 samples per length for illustration\n",
        "        seq = random_sequence(length)\n",
        "        # Randomly decide one-step or two-step transformation\n",
        "        if random.random() < 0.5:\n",
        "            # Single-step: choose F1 or F2 randomly\n",
        "            if random.random() < 0.5:\n",
        "                # ROT13 single-step\n",
        "                prompt_tokens = seq + [T1_TOKEN, ANSWER_TOKEN]\n",
        "                result_seq = apply_rot(seq, n=13)\n",
        "            else:\n",
        "                # POS1 single-step\n",
        "                prompt_tokens = seq + [T2_TOKEN, ANSWER_TOKEN]\n",
        "                result_seq = apply_pos(seq, n=1)\n",
        "            output_tokens = result_seq  # final result only\n",
        "        else:\n",
        "            # Two-step: randomly choose combination of two transforms (allow repeats)\n",
        "            # First transformation:\n",
        "            first_is_rot = random.random() < 0.5\n",
        "            if first_is_rot:\n",
        "                interm_seq = apply_rot(seq, n=13)\n",
        "                first_token = T1_TOKEN\n",
        "            else:\n",
        "                interm_seq = apply_pos(seq, n=1)\n",
        "                first_token = T2_TOKEN\n",
        "            # Second transformation:\n",
        "            second_is_rot = random.random() < 0.5\n",
        "            if second_is_rot:\n",
        "                final_seq = apply_rot(interm_seq, n=13)\n",
        "                second_token = T1_TOKEN\n",
        "            else:\n",
        "                final_seq = apply_pos(interm_seq, n=1)\n",
        "                second_token = T2_TOKEN\n",
        "            # Prompt includes both operations then <think>\n",
        "            prompt_tokens = seq + [first_token, second_token, THINK_TOKEN]\n",
        "            # Output includes intermediate result, second op token, <answer>, then final result\n",
        "            output_tokens = interm_seq + [second_token, ANSWER_TOKEN] + final_seq\n",
        "        # Encode to token ids\n",
        "        input_ids = encode_sequence(prompt_tokens)\n",
        "        output_ids = encode_sequence(output_tokens)\n",
        "        if length == 3:\n",
        "          train_examples3.append((input_ids, output_ids))\n",
        "        elif length == 4:\n",
        "          train_examples4.append((input_ids, output_ids))\n",
        "        elif length == 5:\n",
        "          train_examples5.append((input_ids, output_ids))\n",
        "\n",
        "# Generate evaluation examples for length 1 and 6 (unseen lengths)\n",
        "test_examples_len1 = []\n",
        "test_examples_len6 = []\n",
        "for _ in range(200):  # generate some test examples\n",
        "    seq1 = random_sequence(1)\n",
        "    seq6 = random_sequence(6)\n",
        "    # We'll test on single and double ops for these lengths as well\n",
        "    # Single op for length1\n",
        "    res1 = apply_rot(seq1, 13)\n",
        "    prompt1 = seq1 + [T1_TOKEN, ANSWER_TOKEN]\n",
        "    out1 = res1\n",
        "    test_examples_len1.append((encode_sequence(prompt1), encode_sequence(out1)))\n",
        "    # Two ops for length6\n",
        "    interm6 = apply_rot(seq6, 13)\n",
        "    final6 = apply_pos(interm6, 1)\n",
        "    prompt6 = seq6 + [T1_TOKEN, T2_TOKEN, THINK_TOKEN]  # e.g., first ROT13 then POS1\n",
        "    out6 = interm6 + [T2_TOKEN, ANSWER_TOKEN] + final6\n",
        "    test_examples_len6.append((encode_sequence(prompt6), encode_sequence(out6)))\n"
      ],
      "metadata": {
        "id": "S5AwOmqSyrhP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Example training sample (decoded):\")\n",
        "ex_in, ex_out = random.choice(train_examples3)\n",
        "print(\"Prompt:\", \" \".join(decode_sequence(ex_in)))\n",
        "print(\"Target:\", \" \".join(decode_sequence(ex_out)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MA8FmyMeytsU",
        "outputId": "2df8a256-a36b-4a14-9b86-3557bb814413"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example training sample (decoded):\n",
            "Prompt: Q N G [F2] [F2] <think>\n",
            "Target: N G Q [F2] <answer> G Q N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 109\n",
        "print((train_examples4[i][0]), (train_examples4[i][1]))\n",
        "print(decode_sequence(train_examples4[i][0]), decode_sequence(train_examples4[i][1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDGldeC1tGtl",
        "outputId": "1b27b9b0-fe87-4c5f-b5de-5335ca55c9dc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[16, 9, 15, 10, 27, 27, 28] [9, 15, 10, 16, 27, 29, 15, 10, 16, 9]\n",
            "['Q', 'J', 'P', 'K', '[F2]', '[F2]', '<think>'] ['J', 'P', 'K', 'Q', '[F2]', '<answer>', 'P', 'K', 'Q', 'J']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT2----------"
      ],
      "metadata": {
        "id": "c4C7HpxyHimD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define the PyTorch model\n",
        "class GPTDecoderTorch(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=32, num_heads=4, num_layers=4, d_ff=128):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # Token embedding and positional embedding\n",
        "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_emb = nn.Parameter(torch.zeros(1, 100, d_model))  # max position 100 for example\n",
        "        # Transformer decoder layers\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.TransformerDecoderLayer(d_model, num_heads, dim_feedforward=d_ff, dropout=0.0)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.out_proj = nn.Linear(d_model, vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, x, attn_mask=None):\n",
        "        \"\"\"\n",
        "        x: Tensor of shape (batch, seq_len) of token ids.\n",
        "        attn_mask: Causal mask of shape (seq_len, seq_len) if provided.\n",
        "        \"\"\"\n",
        "        batch, seq_len = x.shape\n",
        "        # Input embeddings\n",
        "        tok_embeddings = self.token_emb(x)  # (batch, seq_len, d_model)\n",
        "        tok_embeddings = tok_embeddings + self.pos_emb[:, :seq_len, :]\n",
        "        # We need to transpose to shape (seq_len, batch, d_model) for PyTorch Transformer\n",
        "        hs = tok_embeddings.transpose(0, 1)  # (seq_len, batch, d_model)\n",
        "        # Pass through each decoder layer (as we are not using an encoder, we treat it as decoder-only)\n",
        "        for layer in self.layers:\n",
        "            hs = layer(hs, hs, tgt_mask=attn_mask)  # decoder layer with no encoder (so using tgt as both)\n",
        "        hs = self.norm(hs)\n",
        "        logits = self.out_proj(hs)  # (seq_len, batch, vocab_size)\n",
        "        return logits.transpose(0, 1)  # return to (batch, seq_len, vocab_size)\n"
      ],
      "metadata": {
        "id": "g32HcisD3zke"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Check this part if it is correctly done!!"
      ],
      "metadata": {
        "id": "9LW7UvDfzYRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_portion = int(len(train_examples3) * 0.85)  # 85% for training\n",
        "test_portion = int(len(train_examples3) * 0.1)    # 10% for testing\n",
        "val_portion = len(train_examples3) - train_portion - test_portion  # Remaining 5% for validation\n",
        "\n",
        "train_data3 = train_examples3[:train_portion]\n",
        "test_data3 = train_examples3[train_portion:train_portion + test_portion]\n",
        "val_data3 = train_examples3[train_portion + test_portion:]\n",
        "\n",
        "train_data4 = train_examples4[:train_portion]\n",
        "test_data4 = train_examples4[train_portion:train_portion + test_portion]\n",
        "val_data4 = train_examples4[train_portion + test_portion:]\n",
        "\n",
        "train_data5 = train_examples5[:train_portion]\n",
        "test_data5 = train_examples5[train_portion:train_portion + test_portion]\n",
        "val_data5 = train_examples5[train_portion + test_portion:]\n",
        "\n",
        "train_data = train_data3 + train_data4 + train_data5\n",
        "test_data = test_data3 + test_data4 + test_data5\n",
        "val_data = val_data3 + val_data4 + val_data5\n",
        "\n",
        "print(\"Training set length:\", len(train_data))\n",
        "print(\"Validation set length:\", len(val_data))\n",
        "print(\"Test set length:\", len(test_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1m64y73--TE",
        "outputId": "545759be-6fef-4cec-b502-9953840e4802"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set length: 25500\n",
            "Validation set length: 1500\n",
            "Test set length: 3000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model and optimizer."
      ],
      "metadata": {
        "id": "NgCFJW--5UW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_torch = GPTDecoderTorch(vocab_size, d_model=32, num_heads=4, num_layers=4, d_ff=4*32)\n",
        "optimizer = torch.optim.Adam(model_torch.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "GhQbCX985TPD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor([[23, 9, 25, 7, 27, 29],[0, 3, 24, 4, 26, 29]])\n",
        "targets = torch.tensor([[9, 25, 7, 27, 29, 9],[3, 24, 4, 26, 29, 13]])"
      ],
      "metadata": {
        "id": "sNgdfJdjIuAF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    logits = model_torch(inputs)\n",
        "probas = torch.softmax(logits, dim=-1)\n",
        "print(probas.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGUusDdNEv8Z",
        "outputId": "e5c4e800-e4f7-41d1-902d-280ba3ab6f98"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 6, 31])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
        "print(\"Token IDs:\\n\", token_ids)\n",
        "\n",
        "print(f\"Targets batch 1: {decode_sequence(targets[0])}\")\n",
        "print(f\"Outputs batch 1: {decode_sequence(token_ids[0].flatten())}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Of7MbBGGIvZ1",
        "outputId": "42cbd5bc-a36c-41bc-d991-6c407d663869"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs:\n",
            " tensor([[[13],\n",
            "         [28],\n",
            "         [13],\n",
            "         [18],\n",
            "         [25],\n",
            "         [18]],\n",
            "\n",
            "        [[19],\n",
            "         [ 8],\n",
            "         [ 2],\n",
            "         [19],\n",
            "         [20],\n",
            "         [18]]])\n",
            "Targets batch 1: ['J', 'Z', 'H', '[F2]', '<answer>', 'J']\n",
            "Outputs batch 1: ['N', '<think>', 'N', 'S', 'Z', 'S']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Logits have shape (batch_size, num_tokens, vocab_size)\n",
        "print(\"Logits shape:\", logits.shape)\n",
        "# Targets have shape (batch_size, num_tokens)\n",
        "print(\"Targets shape:\", targets.shape)\n",
        "\n",
        "logits_flat = logits.flatten(0, 1)\n",
        "targets_flat = targets.flatten()\n",
        "\n",
        "print(\"Flattened logits:\", logits_flat.shape)\n",
        "print(\"Flattened targets:\", targets_flat.shape)\n",
        "\n",
        "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dprOdYBBMEPC",
        "outputId": "473e7528-8dbf-4e73-972e-915e1d84031d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits shape: torch.Size([2, 6, 31])\n",
            "Targets shape: torch.Size([2, 6])\n",
            "Flattened logits: torch.Size([12, 31])\n",
            "Flattened targets: torch.Size([12])\n",
            "tensor(3.6199)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This training is training Decoder Only"
      ],
      "metadata": {
        "id": "a7YFdENKYM7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#train_data\n",
        "\n",
        "def collate(batch, pad_token_id=31, ignore_index=-100):  # batch: list of (prompt_ids, target_ids)\n",
        "    xs, ys, ms = [], [], []\n",
        "    for prompt, target in batch:\n",
        "        seq = prompt + target\n",
        "        x   = seq[:-1]                 # inputs\n",
        "        y   = seq[1:]                  # labels\n",
        "        mask = [0]*(len(prompt)-1) + [1]*len(target)  # no loss on prompt\n",
        "        xs.append(torch.tensor(x)); ys.append(torch.tensor(y)); ms.append(torch.tensor(mask))\n",
        "    L = max(len(x) for x in xs)\n",
        "\n",
        "    # Pad inputs with a different value for diagnosis, but track padding with the original pad_token_id\n",
        "    X_padded_diag = []\n",
        "    padding_mask = []\n",
        "    for t in xs:\n",
        "        padded_t = torch.cat([t, torch.full((L-len(t),), 0, dtype=t.dtype)]) # Pad with 0 for diagnosis\n",
        "        mask_t = torch.cat([torch.full((len(t),), False, dtype=torch.bool), torch.full((L-len(t),), True, dtype=torch.bool)]) # Mask based on original padding intent\n",
        "        X_padded_diag.append(padded_t)\n",
        "        padding_mask.append(mask_t)\n",
        "\n",
        "    X = torch.stack(X_padded_diag)\n",
        "    padding_mask = torch.stack(padding_mask)\n",
        "\n",
        "\n",
        "    # Pad labels with ignore_index\n",
        "    Y = torch.stack([torch.cat([t, torch.full((L-len(t),), ignore_index, dtype=t.dtype)]) for t in ys])\n",
        "\n",
        "    # Pad loss mask with 0\n",
        "    M = torch.stack([torch.cat([t, torch.full((L-len(t),), 0, dtype=t.dtype)]) for t in ms])\n",
        "\n",
        "\n",
        "    return {\"input_ids\": X, \"labels\": Y, \"loss_mask\": M, \"padding_mask\": padding_mask}"
      ],
      "metadata": {
        "id": "yb7Liit_YJnt"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add Ending token at the end."
      ],
      "metadata": {
        "id": "Jph-kbdtwmue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input= collate([train_data[99]]) # Wrap in a list to simulate a batch\n",
        "print(input)\n",
        "collated_data = collate(train_data[:99])\n",
        "input_ids = collated_data['input_ids']\n",
        "labels = collated_data['labels']\n",
        "\n",
        "loss_mask = collated_data['loss_mask']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzNplprk29U_",
        "outputId": "5aa22d8c-aa8f-4b70-f60e-471d8ef4e5e9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[16, 20, 18, 26, 29,  3,  7]]), 'labels': tensor([[20, 18, 26, 29,  3,  7,  5]]), 'loss_mask': tensor([[0, 0, 0, 0, 1, 1, 1]]), 'padding_mask': tensor([[False, False, False, False, False, False, False]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits = model_torch(input_ids)                 # (B, L, V)\n",
        "logits2D = logits.reshape(-1, logits.size(-1))  # (B*L, V)\n",
        "targets1D = labels.reshape(-1)                  # (B*L,)\n",
        "mask1D    = loss_mask.reshape(-1).float()       # (B*L,)\n",
        "\n",
        "loss_all = torch.nn.functional.cross_entropy(logits2D, targets1D, reduction=\"none\")\n",
        "loss = (loss_all * mask1D).sum() / mask1D.sum()\n",
        "\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuxPS2z72JUF",
        "outputId": "ae24926c-6509-474d-c706-88a7197a853c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.5932, grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss(input_ids,labels,loss_mask):\n",
        "    logits = model_torch(input_ids)                 # (B, L, V)\n",
        "    logits2D = logits.reshape(-1, logits.size(-1))  # (B*L, V)\n",
        "    targets1D = labels.reshape(-1)                  # (B*L,)\n",
        "    mask1D    = loss_mask.reshape(-1).float()       # (B*L,)\n",
        "\n",
        "    loss_all = torch.nn.functional.cross_entropy(logits2D, targets1D, reduction=\"none\")\n",
        "    loss = (loss_all * mask1D).sum() / mask1D.sum()\n",
        "    return loss"
      ],
      "metadata": {
        "id": "iIneEasleycC"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is training loop(fix)"
      ],
      "metadata": {
        "id": "DGmrIUgm4nZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility: create causal attn mask for PyTorch (size [seq_len, seq_len])\n",
        "def generate_causal_mask(seq_len):\n",
        "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)  # 1s above diagonal\n",
        "    mask = mask.masked_fill(mask == 1, float('-inf'))  # convert to -inf where mask is 1 (to block)\n",
        "    return mask  # PyTorch uses -inf for masked positions"
      ],
      "metadata": {
        "id": "Vym1Y7hdJcs6"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maybe add txt genaration function too"
      ],
      "metadata": {
        "id": "Q6ImxMyqlU9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "collated_data = collate(train_data)\n",
        "\n",
        "input_ids = collated_data['input_ids']\n",
        "labels = collated_data['labels']\n",
        "loss_mask = collated_data['loss_mask']\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# Training loop\n",
        "model_torch.train()\n",
        "num_epochs = 5\n",
        "batch_size = 64\n",
        "num_samples = input_ids.size(0)\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    # Shuffle indices\n",
        "    indices = torch.randperm(num_samples)\n",
        "    for i in range(0, num_samples, batch_size):\n",
        "        idx = indices[i:i+batch_size]\n",
        "        batch_in = input_ids[idx]\n",
        "        batch_lbl = labels[idx]\n",
        "        batch_mask = loss_mask[idx]\n",
        "        seq_len = batch_in.size(1)\n",
        "\n",
        "        # Causal mask for this sequence length\n",
        "        attn_mask = generate_causal_mask(seq_len)\n",
        "        logits = model_torch(batch_in, attn_mask=attn_mask.to(batch_in.device))\n",
        "\n",
        "        # Compute loss\n",
        "        loss = calc_loss(input_ids,labels,loss_mask)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch} done (last batch loss = {loss.item():.4f}).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "4DKE_0UL4pyv",
        "outputId": "c3dfc104-6d32-4f98-eadf-95c70fe99293"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3118764969.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch} done (last batch loss = {loss.item():.4f}).\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model_torch.state_dict(), \"model.pth\")"
      ],
      "metadata": {
        "id": "NVMaagcYli9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the accuracy!!"
      ],
      "metadata": {
        "id": "qk-1TPGA4x2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_torch.eval()\n",
        "\n",
        "def greedy_decode_torch(prompt_ids, max_len):\n",
        "    generated = prompt_ids.clone()  # start with the prompt\n",
        "    for _ in range(max_len):\n",
        "        seq_len = generated.size(1)\n",
        "        attn_mask = generate_causal_mask(seq_len).to(generated.device)\n",
        "        with torch.no_grad():\n",
        "            logits = model_torch(generated, attn_mask=attn_mask)\n",
        "        next_token = int(torch.argmax(logits[0, -1]))\n",
        "        # Append next token\n",
        "        next_tok_t = torch.tensor([[next_token]], dtype=torch.long)\n",
        "        generated = torch.cat([generated, next_tok_t], dim=1)\n",
        "        if next_token == token_to_id[ANSWER_TOKEN]:\n",
        "            # Continue until after answer token to get final answer,\n",
        "            # stopping criteria could also be sequence length or a special end token if defined.\n",
        "            continue\n",
        "    # Return generated part after the prompt\n",
        "    return generated[0, prompt_ids.size(1):].tolist()\n",
        "\n",
        "# Evaluate accuracy on test sets\n",
        "for test_set, name in [(test_examples_len1, \"Length-1\"), (test_examples_len6, \"Length-6\")]:\n",
        "    correct = 0\n",
        "    total = len(test_set)\n",
        "    for inp_ids, true_out_ids in test_set:\n",
        "        inp_t = torch.tensor([inp_ids], dtype=torch.long)\n",
        "        gen_ids = greedy_decode_torch(inp_t, max_len=len(true_out_ids)+5)\n",
        "        gen_ids = gen_ids[:len(true_out_ids)]\n",
        "        if gen_ids == true_out_ids:\n",
        "            correct += 1\n",
        "    print(f\"{name} exact match accuracy: {100 * correct/total:.2f}%\")"
      ],
      "metadata": {
        "id": "v9xnetRpy4Cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decode_sequence(test_set[1][0])"
      ],
      "metadata": {
        "id": "ktang4kzuKbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode_sequence(test_set[1][1]))\n",
        "print(test_set[1])"
      ],
      "metadata": {
        "id": "XODuzfJruUZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "greedy_decode(test_set[1][0],max_len=100)"
      ],
      "metadata": {
        "id": "nBzxi2q6rJ-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b=99\n",
        "print(decode_sequence(greedy_decode(test_set[b][0],max_len=100)))\n",
        "print(decode_sequence(test_set[b][1]))"
      ],
      "metadata": {
        "id": "aXiZ4H9iuycl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next is testing len 3, 4 and 5"
      ],
      "metadata": {
        "id": "u46TssVsahUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_examples_len3 = []\n",
        "test_examples_len4 = []\n",
        "test_examples_len5 = []\n",
        "for _ in range(200):  # generate some test examples\n",
        "    seq3 = random_sequence(3)\n",
        "    seq4 = random_sequence(4)\n",
        "    seq5 = random_sequence(5)\n",
        "\n",
        "    # Two ops for each length\n",
        "    interm3 = apply_rot(seq3, 13)\n",
        "    final3 = apply_pos(interm3, 1)\n",
        "    prompt3 = seq3 + [T1_TOKEN, T2_TOKEN, THINK_TOKEN]  # e.g., first ROT13 then POS1\n",
        "    out3 = interm3 + [T2_TOKEN, ANSWER_TOKEN] + final3\n",
        "    test_examples_len3.append((encode_sequence(prompt3), encode_sequence(out3)))\n",
        "\n",
        "    interm4 = apply_rot(seq4, 13)\n",
        "    final4 = apply_pos(interm4, 1)\n",
        "    prompt4 = seq4 + [T1_TOKEN, T2_TOKEN, THINK_TOKEN]  # e.g., first ROT13 then POS1\n",
        "    out4 = interm4 + [T2_TOKEN, ANSWER_TOKEN] + final4\n",
        "    test_examples_len4.append((encode_sequence(prompt4), encode_sequence(out4)))\n",
        "\n",
        "    interm5 = apply_rot(seq5, 13)\n",
        "    final5 = apply_pos(interm5, 1)\n",
        "    prompt5 = seq5 + [T1_TOKEN, T2_TOKEN, THINK_TOKEN]  # e.g., first ROT13 then POS1\n",
        "    out5 = interm5 + [T2_TOKEN, ANSWER_TOKEN] + final5\n",
        "    test_examples_len5.append((encode_sequence(prompt5), encode_sequence(out5)))"
      ],
      "metadata": {
        "id": "yTymWtfkaswV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate exact match on test examples\n",
        "for test_set, name in [(test_examples_len3, \"Length-3\"), (test_examples_len4, \"Length-4\"), (test_examples_len6, \"Length-5\")]:\n",
        "    correct = 0\n",
        "    total = len(test_set)\n",
        "    for inp_ids, true_out_ids in test_set:\n",
        "        # decode until we produce as many tokens as true_out (or a bit more)\n",
        "        gen_ids = greedy_decode(inp_ids, max_len=len(true_out_ids)+5)\n",
        "        # Compare with true output\n",
        "        # Note: need to stop at the same length as true output\n",
        "        gen_ids = gen_ids[:len(true_out_ids)]\n",
        "        if gen_ids == true_out_ids:\n",
        "            correct += 1\n",
        "    print(f\"{name} exact match accuracy: {100 * correct/total:.2f}%\")"
      ],
      "metadata": {
        "id": "CRThGjLSZ7jW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OLpIgYJsd9WA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "concat = train_examples[1][0]+train_examples[1][1]\n",
        "concat\n",
        "\n",
        "concat[:-1]\n",
        "for i in range(10):\n",
        "  print(train_inputs[i],train_labels[i])\n",
        "\n",
        "for i in range(10):\n",
        "  print(decode_sequence(train_inputs[i]),decode_sequence(train_labels[i]))"
      ],
      "metadata": {
        "id": "RPeRMAw6LCSs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}