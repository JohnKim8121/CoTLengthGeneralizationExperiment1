{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3CQCkfpvm0pu2YZpkjO7D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JohnKim8121/CoTLengthGeneralizationExperiment1/blob/main/Experiment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "P7rJVosdyk-T"
      },
      "outputs": [],
      "source": [
        "# Define the vocabulary and encoding/decoding\n",
        "import string\n",
        "\n",
        "# Base alphabet tokens\n",
        "alphabet = list(string.ascii_uppercase)  # ['A','B',...,'Z']\n",
        "# Special tokens\n",
        "T1_TOKEN = \"[F1]\"   # denotes ROT13\n",
        "T2_TOKEN = \"[F2]\"   # denotes POS1\n",
        "THINK_TOKEN = \"<think>\"\n",
        "ANSWER_TOKEN = \"<answer>\"\n",
        "\n",
        "# Construct vocabulary list\n",
        "vocab = alphabet + [T1_TOKEN, T2_TOKEN, THINK_TOKEN, ANSWER_TOKEN]\n",
        "vocab_size = len(vocab)\n",
        "token_to_id = {tok: i for i, tok in enumerate(vocab)}\n",
        "id_to_token = {i: tok for tok, i in token_to_id.items()}\n",
        "\n",
        "def encode_sequence(seq_tokens):\n",
        "    \"\"\"Encode a sequence of token strings (letters or special markers) to list of ids.\"\"\"\n",
        "    return [token_to_id[token] for token in seq_tokens]\n",
        "\n",
        "def decode_sequence(id_list):\n",
        "    \"\"\"Decode a list of token ids back to token strings.\"\"\"\n",
        "    return [id_to_token[i] for i in id_list]\n",
        "\n",
        "# Transformation functions\n",
        "def apply_rot(sequence, n=13):\n",
        "    \"\"\"Apply ROT-n to a sequence of letters (list of chars).\"\"\"\n",
        "    result = []\n",
        "    for ch in sequence:\n",
        "        if ch not in token_to_id or ch not in alphabet:\n",
        "            raise ValueError(f\"Unexpected token in sequence: {ch}\")\n",
        "        # shift letter by n\n",
        "        new_idx = (ord(ch) - ord('A') + n) % 26\n",
        "        result.append(chr(ord('A') + new_idx))\n",
        "    return result\n",
        "\n",
        "def apply_pos(sequence, n=1):\n",
        "    \"\"\"Apply cyclic position shift (left rotate by n) to a sequence of letters.\"\"\"\n",
        "    l = len(sequence)\n",
        "    # left rotation by n: element at index i moves to index i-n (mod l) in the result\n",
        "    return [sequence[(i + n) % l] for i in range(l)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment 1 is training len 3,4,5 to test len 1 and 6."
      ],
      "metadata": {
        "id": "qlKmtse9ZMla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Function to generate a random sequence of given length\n",
        "def random_sequence(length):\n",
        "    return [random.choice(alphabet) for _ in range(length)]\n",
        "\n",
        "# Generate training examples\n",
        "train_examples = []\n",
        "train_lengths = [3, 4, 5]  # in-distribution sequence lengths\n",
        "for length in train_lengths:\n",
        "    # For each length, generate a number of examples (you can adjust count for real training)\n",
        "    for _ in range(1000):  # e.g., 1000 samples per length for illustration\n",
        "        seq = random_sequence(length)\n",
        "        # Randomly decide one-step or two-step transformation\n",
        "        if random.random() < 0.5:\n",
        "            # Single-step: choose F1 or F2 randomly\n",
        "            if random.random() < 0.5:\n",
        "                # ROT13 single-step\n",
        "                prompt_tokens = seq + [T1_TOKEN, ANSWER_TOKEN]\n",
        "                result_seq = apply_rot(seq, n=13)\n",
        "            else:\n",
        "                # POS1 single-step\n",
        "                prompt_tokens = seq + [T2_TOKEN, ANSWER_TOKEN]\n",
        "                result_seq = apply_pos(seq, n=1)\n",
        "            output_tokens = result_seq  # final result only\n",
        "        else:\n",
        "            # Two-step: randomly choose combination of two transforms (allow repeats)\n",
        "            # First transformation:\n",
        "            first_is_rot = random.random() < 0.5\n",
        "            if first_is_rot:\n",
        "                interm_seq = apply_rot(seq, n=13)\n",
        "                first_token = T1_TOKEN\n",
        "            else:\n",
        "                interm_seq = apply_pos(seq, n=1)\n",
        "                first_token = T2_TOKEN\n",
        "            # Second transformation:\n",
        "            second_is_rot = random.random() < 0.5\n",
        "            if second_is_rot:\n",
        "                final_seq = apply_rot(interm_seq, n=13)\n",
        "                second_token = T1_TOKEN\n",
        "            else:\n",
        "                final_seq = apply_pos(interm_seq, n=1)\n",
        "                second_token = T2_TOKEN\n",
        "            # Prompt includes both operations then <think>\n",
        "            prompt_tokens = seq + [first_token, second_token, THINK_TOKEN]\n",
        "            # Output includes intermediate result, second op token, <answer>, then final result\n",
        "            output_tokens = interm_seq + [second_token, ANSWER_TOKEN] + final_seq\n",
        "        # Encode to token ids\n",
        "        input_ids = encode_sequence(prompt_tokens)\n",
        "        output_ids = encode_sequence(output_tokens)\n",
        "        train_examples.append((input_ids, output_ids))\n",
        "\n",
        "# Generate evaluation examples for length 1 and 6 (unseen lengths)\n",
        "test_examples_len1 = []\n",
        "test_examples_len6 = []\n",
        "for _ in range(200):  # generate some test examples\n",
        "    seq1 = random_sequence(1)\n",
        "    seq6 = random_sequence(6)\n",
        "    # We'll test on single and double ops for these lengths as well\n",
        "    # Single op for length1\n",
        "    res1 = apply_rot(seq1, 13)\n",
        "    prompt1 = seq1 + [T1_TOKEN, ANSWER_TOKEN]\n",
        "    out1 = res1\n",
        "    test_examples_len1.append((encode_sequence(prompt1), encode_sequence(out1)))\n",
        "    # Two ops for length6\n",
        "    interm6 = apply_rot(seq6, 13)\n",
        "    final6 = apply_pos(interm6, 1)\n",
        "    prompt6 = seq6 + [T1_TOKEN, T2_TOKEN, THINK_TOKEN]  # e.g., first ROT13 then POS1\n",
        "    out6 = interm6 + [T2_TOKEN, ANSWER_TOKEN] + final6\n",
        "    test_examples_len6.append((encode_sequence(prompt6), encode_sequence(out6)))\n"
      ],
      "metadata": {
        "id": "S5AwOmqSyrhP"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Example training sample (decoded):\")\n",
        "ex_in, ex_out = random.choice(train_examples)\n",
        "print(\"Prompt:\", \" \".join(decode_sequence(ex_in)))\n",
        "print(\"Target:\", \" \".join(decode_sequence(ex_out)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MA8FmyMeytsU",
        "outputId": "0d61b736-ca0e-40ba-c22f-e00e30603c4d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example training sample (decoded):\n",
            "Prompt: K O J T [F2] <answer>\n",
            "Target: O J T K\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 99\n",
        "print((train_examples[i][0]), (train_examples[i][1]))\n",
        "print(decode_sequence(train_examples[i][0]), decode_sequence(train_examples[i][1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDGldeC1tGtl",
        "outputId": "f7d2ab38-3ba6-4d41-93c8-20c65c98097c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4, 11, 25, 26, 27, 28] [17, 24, 12, 27, 29, 24, 12, 17]\n",
            "['E', 'L', 'Z', '[F1]', '[F2]', '<think>'] ['R', 'Y', 'M', '[F2]', '<answer>', 'Y', 'M', 'R']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flax optax\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3g5FhGgywIA",
        "outputId": "3205c241-ef5e-4d45-be41-43971847135f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flax in /usr/local/lib/python3.12/dist-packages (0.10.6)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.12/dist-packages (0.2.5)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.12/dist-packages (from flax) (2.0.2)\n",
            "Requirement already satisfied: jax>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from flax) (0.5.3)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.12/dist-packages (from flax) (1.1.1)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.12/dist-packages (from flax) (0.11.22)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.12/dist-packages (from flax) (0.1.76)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.12/dist-packages (from flax) (13.9.4)\n",
            "Requirement already satisfied: typing_extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from flax) (4.14.1)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.12/dist-packages (from flax) (6.0.2)\n",
            "Requirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.12/dist-packages (from flax) (0.1.10)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from optax) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.12/dist-packages (from optax) (0.1.90)\n",
            "Requirement already satisfied: jaxlib>=0.4.27 in /usr/local/lib/python3.12/dist-packages (from optax) (0.5.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax) (75.2.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax) (0.12.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from jax>=0.5.1->flax) (0.5.3)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax>=0.5.1->flax) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.12/dist-packages (from jax>=0.5.1->flax) (1.16.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax) (2.19.2)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax) (1.13.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax) (1.6.0)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax) (24.1.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax) (5.29.5)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax) (4.12.3)\n",
            "Requirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax) (3.20.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax) (0.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (2025.3.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (3.23.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flax import linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from functools import partial\n",
        "\n",
        "# Model hyperparameters\n",
        "d_model = 32        # embedding/hidden size\n",
        "num_heads = 4       # number of attention heads\n",
        "num_layers = 4      # number of transformer decoder layers\n",
        "d_ff = 4 * d_model  # feed-forward hidden dim (e.g. 128)\n",
        "dropout_rate = 0.0  # (for simplicity, we disable dropout in this example)\n",
        "\n",
        "class TransformerDecoderBlock(nn.Module):\n",
        "    \"\"\"Single transformer decoder block with causal self-attention.\"\"\"\n",
        "    @nn.compact\n",
        "    def __call__(self, x, train=True, attn_mask=None):\n",
        "        # Layer norm\n",
        "        norm1 = nn.LayerNorm()(x)\n",
        "        # Self-attention (causal)\n",
        "        attn = nn.SelfAttention(num_heads=num_heads,\n",
        "                                qkv_features=d_model,\n",
        "                                use_bias=True,\n",
        "                                broadcast_dropout=False,\n",
        "                                deterministic=not train)(norm1, mask=attn_mask)\n",
        "        x = x + attn  # residual connection\n",
        "        # Second layer norm and feed-forward MLP\n",
        "        norm2 = nn.LayerNorm()(x)\n",
        "        ff = nn.Dense(d_ff)(norm2)\n",
        "        ff = nn.gelu(ff)\n",
        "        ff = nn.Dense(d_model)(ff)\n",
        "        x = x + ff  # residual connection\n",
        "        return x\n",
        "\n",
        "class GPTDecoder(nn.Module):\n",
        "    \"\"\"Small GPT-like decoder model.\"\"\"\n",
        "\n",
        "    max_len: int  # new argument\n",
        "    @nn.compact\n",
        "    def __call__(self, input_ids, train=True):\n",
        "        # input_ids: [batch, seq_length]\n",
        "        batch, seq_len = input_ids.shape\n",
        "        # Token embeddings\n",
        "        embed_init = nn.initializers.normal(stddev=0.02)\n",
        "        token_embed = nn.Embed(num_embeddings=vocab_size, features=d_model,\n",
        "                               embedding_init=embed_init)\n",
        "        x = token_embed(input_ids)  # shape [batch, seq_len, d_model]\n",
        "        # Positional embeddings (learned)\n",
        "        pos_embed = self.param(\"pos_embedding\", nn.initializers.normal(stddev=0.01),\n",
        "                               (1, self.max_len, d_model))\n",
        "        x = x + pos_embed[:, :seq_len, :]\n",
        "        # Apply multiple Transformer decoder blocks\n",
        "        # Causal mask: allow attention only to current and past positions\n",
        "        # mask shape: [batch, 1, seq_len, seq_len]\n",
        "        if attn_mask := True:  # (will define mask below outside Module for JIT)\n",
        "            pass\n",
        "        attn_mask = None  # placeholder, see below on how we pass mask\n",
        "        for _ in range(num_layers):\n",
        "            x = TransformerDecoderBlock()(x, train=train, attn_mask=attn_mask)\n",
        "        # Final layer norm\n",
        "        x = nn.LayerNorm()(x)\n",
        "        # Output projection\n",
        "        logits = nn.Dense(vocab_size, use_bias=False)(x)\n",
        "        return logits\n",
        "\n",
        "# Create causal mask function\n",
        "def causal_mask(seq_len):\n",
        "    # Mask shape [1, 1, seq_len, seq_len] with True allowed positions\n",
        "    # We want to mask out future => allow indices i >= j\n",
        "    mask = jnp.tril(jnp.ones((seq_len, seq_len), dtype=jnp.bool_), k=0)\n",
        "    # Add batch and head dimensions: shape (batch, heads, seq, seq)\n",
        "    return mask.reshape(1, 1, seq_len, seq_len)"
      ],
      "metadata": {
        "id": "g32HcisD3zke"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "concat = train_examples[1][0]+train_examples[1][1]\n",
        "concat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Bl--cLJvRjd",
        "outputId": "2c4278fe-ec22-4891-811e-4eed13dd6565"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 9, 15, 26, 29, 16, 22, 2]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "concat[:-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPQV3Hquvmlx",
        "outputId": "cf203a4a-2155-42e7-cdd2-32d32f129f98"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 9, 15, 26, 29, 16, 22]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "concat[1:]\n",
        "for i in range(10):\n",
        "  print(train_inputs[i],train_labels[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnuI7Uktv0UH",
        "outputId": "5daa6583-f411-4419-fc25-779c4b3ecc3b"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 4  1  7 26 27 28 17 14 20 27 29 14 20  0  0  0  0  0  0] [ 1  7 26 27 28 17 14 20 27 29 14 20 17  0  0  0  0  0  0]\n",
            "[ 3  9 15 26 29 16 22  0  0  0  0  0  0  0  0  0  0  0  0] [ 9 15 26 29 16 22  2  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "[11 14 16 27 29 14 16  0  0  0  0  0  0  0  0  0  0  0  0] [14 16 27 29 14 16 11  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "[20  7 17 26 29  7 20  0  0  0  0  0  0  0  0  0  0  0  0] [ 7 17 26 29  7 20  4  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "[ 8 23 12 26 27 28 21 10 25 27 29 10 25  0  0  0  0  0  0] [23 12 26 27 28 21 10 25 27 29 10 25 21  0  0  0  0  0  0]\n",
            "[ 9  6  8 26 29 22 19  0  0  0  0  0  0  0  0  0  0  0  0] [ 6  8 26 29 22 19 21  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "[21 10  9 26 27 28  8 23 22 27 29 23 22  0  0  0  0  0  0] [10  9 26 27 28  8 23 22 27 29 23 22  8  0  0  0  0  0  0]\n",
            "[ 9 22  0 26 27 28 22  9 13 27 29  9 13  0  0  0  0  0  0] [22  0 26 27 28 22  9 13 27 29  9 13 22  0  0  0  0  0  0]\n",
            "[19 21 20 26 26 28  6  8  7 26 29 19 21  0  0  0  0  0  0] [21 20 26 26 28  6  8  7 26 29 19 21 20  0  0  0  0  0  0]\n",
            "[ 6 10 18 27 27 28 10 18  6 27 29 18  6  0  0  0  0  0  0] [10 18 27 27 28 10 18  6 27 29 18  6 10  0  0  0  0  0  0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Check this part if it is correctly done!!"
      ],
      "metadata": {
        "id": "9LW7UvDfzYRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optax\n",
        "\n",
        "# Prepare training data as concatenated sequences and loss masks\n",
        "train_inputs = []\n",
        "train_labels = []\n",
        "loss_masks = []  # 1 for tokens where loss should be applied, 0 for prompt tokens\n",
        "\n",
        "for input_ids, output_ids in train_examples:\n",
        "    concat = input_ids + output_ids  # concatenate prompt + target\n",
        "    train_inputs.append(concat[:-1])   # model input: all except last token (next-token pred)\n",
        "    train_labels.append(concat[1:])    # expected outputs: all except first token\n",
        "    # Mask: 0 for prompt part labels, 1 for output part labels\n",
        "    prompt_len = len(input_ids)\n",
        "    seq_len = len(concat) - 1\n",
        "    mask = [0]* (prompt_len - 1) + [1]* (len(concat) - prompt_len)\n",
        "    # (We put 0 for prompt tokens *except* the last prompt token because once we reach <think>/<answer>,\n",
        "    # the next token to predict is the first token of reasoning, which we do want to learn.\n",
        "    # The above scheme masks all prompt indices except we allow starting from the first output token.)\n",
        "    loss_masks.append(mask)\n",
        "\n",
        "# Pad sequences to same length for batching\n",
        "max_len = max(len(seq) for seq in train_inputs)\n",
        "def pad(seq, length, pad_id=0):\n",
        "    return seq + [pad_id] * (length - len(seq))\n",
        "train_inputs = [pad(seq, max_len) for seq in train_inputs]\n",
        "train_labels = [pad(seq, max_len) for seq in train_labels]\n",
        "loss_masks = [pad(mask, max_len, pad_id=0) for mask in loss_masks]\n",
        "\n",
        "train_inputs = jnp.array(train_inputs)\n",
        "train_labels = jnp.array(train_labels)\n",
        "loss_masks = jnp.array(loss_masks)\n",
        "\n",
        "# Initialize model and optimizer\n",
        "model = GPTDecoder(max_len=30)\n",
        "params = model.init(jax.random.PRNGKey(0), train_inputs[:1], train=True)[\"params\"]\n",
        "optimizer = optax.adam(learning_rate=1e-3)\n",
        "opt_state = optimizer.init(params)\n",
        "\n",
        "# Define loss function\n",
        "def compute_loss(params, batch_inputs, batch_labels, batch_mask):\n",
        "    logits = model.apply({\"params\": params}, batch_inputs, train=True)\n",
        "    # Compute cross-entropy\n",
        "    num_classes = logits.shape[-1]\n",
        "    # Flatten for convenience:\n",
        "    logits_flat = logits.reshape(-1, num_classes)\n",
        "    labels_flat = batch_labels.reshape(-1,)\n",
        "    mask_flat = batch_mask.reshape(-1,)\n",
        "    # Optax cross_entropy_with_integer_labels expects unnormalized logits\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(logits_flat, labels_flat)\n",
        "    # Apply mask to zero-out loss for prompt tokens\n",
        "    loss = loss * mask_flat\n",
        "    # Average loss\n",
        "    return loss.sum() / mask_flat.sum()\n",
        "\n",
        "# Training loop (simple form)\n",
        "num_epochs = 5\n",
        "batch_size = 64\n",
        "num_samples = train_inputs.shape[0]\n",
        "\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    # Shuffle training data\n",
        "    permutation = jax.random.permutation(jax.random.PRNGKey(epoch), num_samples)\n",
        "    shuffled_inputs = train_inputs[permutation]\n",
        "    shuffled_labels = train_labels[permutation]\n",
        "    shuffled_masks = loss_masks[permutation]\n",
        "    # Batch iteration\n",
        "    for i in range(0, num_samples, batch_size):\n",
        "        batch_in = shuffled_inputs[i:i+batch_size]\n",
        "        batch_lbl = shuffled_labels[i:i+batch_size]\n",
        "        batch_m = shuffled_masks[i:i+batch_size]\n",
        "        # Compute gradients\n",
        "        loss_val, grads = jax.value_and_grad(compute_loss)(params, batch_in, batch_lbl, batch_m)\n",
        "        # Update params\n",
        "        updates, opt_state = optimizer.update(grads, opt_state, params)\n",
        "        params = optax.apply_updates(params, updates)\n",
        "    print(f\"Epoch {epoch} done.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jps9bqEy1dk",
        "outputId": "1ef3b81b-b080-4f22-fd8f-f64165e7fd55"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 done.\n",
            "Epoch 2 done.\n",
            "Epoch 3 done.\n",
            "Epoch 4 done.\n",
            "Epoch 5 done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_decode(prompt_ids, max_len):\n",
        "    \"\"\"Generate output from model given prompt (without including the prompt termination token).\"\"\"\n",
        "    # Start with prompt as context\n",
        "    generated = list(prompt_ids)\n",
        "    # We assume prompt already contains <think> or <answer> token at end to signal start of gen.\n",
        "    for _ in range(max_len):\n",
        "        inputs = jnp.array([generated])  # shape (1, cur_len)\n",
        "        # Create causal mask for the current sequence length\n",
        "        current_seq_len = inputs.shape[1]\n",
        "        # The causal mask is created inside the model now, no need to pass it here.\n",
        "        # causal_mask = create_causal_mask(current_seq_len) if current_seq_len > 1 else None\n",
        "\n",
        "        logits = model.apply({\"params\": params}, inputs, train=False)  # get logits\n",
        "        next_id = int(jnp.argmax(logits[0, -1]))  # pick highest probability token\n",
        "        generated.append(next_id)\n",
        "        # Stop if <answer> was just produced and we have some output after it (meaning chain ended)\n",
        "        if next_id == token_to_id[ANSWER_TOKEN] and len(generated) > len(prompt_ids) + 1:\n",
        "            break\n",
        "    return generated[len(prompt_ids):]  # return the generated portion (excluding prompt)\n",
        "\n",
        "# Evaluate exact match on test examples\n",
        "for test_set, name in [(test_examples_len1, \"Length-1\"), (test_examples_len6, \"Length-6\")]:\n",
        "    correct = 0\n",
        "    total = len(test_set)\n",
        "    for inp_ids, true_out_ids in test_set:\n",
        "        # decode until we produce as many tokens as true_out (or a bit more)\n",
        "        gen_ids = greedy_decode(inp_ids, max_len=len(true_out_ids)+5)\n",
        "        # Compare with true output\n",
        "        # Note: need to stop at the same length as true output\n",
        "        gen_ids = gen_ids[:len(true_out_ids)]\n",
        "        if gen_ids == true_out_ids:\n",
        "            correct += 1\n",
        "    print(f\"{name} exact match accuracy: {100 * correct/total:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9xnetRpy4Cn",
        "outputId": "0e336cf0-7345-45a1-e5e7-e28702a4b1dd"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length-1 exact match accuracy: 8.00%\n",
            "Length-6 exact match accuracy: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decode_sequence(test_set[1][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktang4kzuKbN",
        "outputId": "34565ae8-98ca-472f-d495-6c1a9d561ce2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['R', 'X', 'B', 'A', 'U', 'Q', '[F1]', '[F2]', '<think>']"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode_sequence(test_set[1][1]))\n",
        "print(test_set[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XODuzfJruUZ7",
        "outputId": "814de058-9349-4657-f818-102e1c46d1ae"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['E', 'K', 'O', 'N', 'H', 'D', '[F2]', '<answer>', 'K', 'O', 'N', 'H', 'D', 'E']\n",
            "([17, 23, 1, 0, 20, 16, 26, 27, 28], [4, 10, 14, 13, 7, 3, 27, 29, 10, 14, 13, 7, 3, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "greedy_decode(test_set[1][0],max_len=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBzxi2q6rJ-p",
        "outputId": "be6b17e9-eb83-4e41-b4e4-82c39c340d5d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[16, 16, 17, 16, 26, 29]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b=99\n",
        "print(decode_sequence(greedy_decode(test_set[b][0],max_len=100)))\n",
        "print(decode_sequence(test_set[b][1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXiZ4H9iuycl",
        "outputId": "a321b9b6-e5da-422e-a00d-34532bb03c05"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['X', 'H', 'L', 'A', '[F1]', '<answer>']\n",
            "['T', 'C', 'V', 'B', 'U', 'K', '[F2]', '<answer>', 'C', 'V', 'B', 'U', 'K', 'T']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next is testing len 3, 4 and 5"
      ],
      "metadata": {
        "id": "u46TssVsahUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_examples_len3 = []\n",
        "test_examples_len4 = []\n",
        "test_examples_len5 = []\n",
        "for _ in range(200):  # generate some test examples\n",
        "    seq3 = random_sequence(3)\n",
        "    seq4 = random_sequence(4)\n",
        "    seq5 = random_sequence(5)\n",
        "\n",
        "    # Two ops for each length\n",
        "    interm3 = apply_rot(seq3, 13)\n",
        "    final3 = apply_pos(interm3, 1)\n",
        "    prompt3 = seq3 + [T1_TOKEN, T2_TOKEN, THINK_TOKEN]  # e.g., first ROT13 then POS1\n",
        "    out3 = interm3 + [T2_TOKEN, ANSWER_TOKEN] + final3\n",
        "    test_examples_len3.append((encode_sequence(prompt3), encode_sequence(out3)))\n",
        "\n",
        "    interm4 = apply_rot(seq4, 13)\n",
        "    final4 = apply_pos(interm4, 1)\n",
        "    prompt4 = seq4 + [T1_TOKEN, T2_TOKEN, THINK_TOKEN]  # e.g., first ROT13 then POS1\n",
        "    out4 = interm4 + [T2_TOKEN, ANSWER_TOKEN] + final4\n",
        "    test_examples_len4.append((encode_sequence(prompt4), encode_sequence(out4)))\n",
        "\n",
        "    interm5 = apply_rot(seq5, 13)\n",
        "    final5 = apply_pos(interm5, 1)\n",
        "    prompt5 = seq5 + [T1_TOKEN, T2_TOKEN, THINK_TOKEN]  # e.g., first ROT13 then POS1\n",
        "    out5 = interm5 + [T2_TOKEN, ANSWER_TOKEN] + final5\n",
        "    test_examples_len5.append((encode_sequence(prompt5), encode_sequence(out5)))"
      ],
      "metadata": {
        "id": "yTymWtfkaswV"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate exact match on test examples\n",
        "for test_set, name in [(test_examples_len3, \"Length-3\"), (test_examples_len4, \"Length-4\"), (test_examples_len6, \"Length-5\")]:\n",
        "    correct = 0\n",
        "    total = len(test_set)\n",
        "    for inp_ids, true_out_ids in test_set:\n",
        "        # decode until we produce as many tokens as true_out (or a bit more)\n",
        "        gen_ids = greedy_decode(inp_ids, max_len=len(true_out_ids)+5)\n",
        "        # Compare with true output\n",
        "        # Note: need to stop at the same length as true output\n",
        "        gen_ids = gen_ids[:len(true_out_ids)]\n",
        "        if gen_ids == true_out_ids:\n",
        "            correct += 1\n",
        "    print(f\"{name} exact match accuracy: {100 * correct/total:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRThGjLSZ7jW",
        "outputId": "e2551011-ff81-4ef3-bec9-52f97479ce14"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length-3 exact match accuracy: 0.00%\n",
            "Length-4 exact match accuracy: 0.00%\n",
            "Length-5 exact match accuracy: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OLpIgYJsd9WA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}